# Use Case 4: 위험 시그널 조기 감지 - PM 전략 검증

**문서 버전**: v1.0
**작성일**: 2026-01-07
**PM**: Terry
**검증 프레임워크**: Evidence-Guided (Itamar Gilad)
**기반 문서**: ontology-pm-strategy.md v3.0

---

## Executive Summary

### 최종 판단: HOLD - Phase 2로 이동 권장

| 평가 항목 | 결과 | 신뢰도 |
|----------|------|--------|
| **고객 검증** | **0/92 질문** (0%) | High |
| **온톨로지 필요성** | 검증됨 (복잡한 유사도 계산 필수) | High |
| **비즈니스 임팩트** | 잠재적으로 높음 (Bad Hire 방지) | Low (가정) |
| **데이터 준비도** | 부족 (Skill Object 데이터 없음) | High |
| **리스크 수준** | **매우 높음** (법적/윤리적 지뢰밭) | High |

**권장사항**:
- Use Case 1, 2만 MVP 포함 (3개월 범위)
- Use Case 4는 Phase 2 (MVP+6개월) 이동
- MVP 기간 중 고객 검증 및 법무 검토 선행

---

## 1. MVP 우선순위 검증

### 1.1 Evidence Readiness 체크 (Evidence-Guided 프레임워크)

| Evidence 유형 | Use Case 1 (리드타임) | Use Case 2 (유사 후보자) | Use Case 4 (위험 시그널) |
|--------------|----------------------|------------------------|------------------------|
| **고객 시그널** | ✅ 원티드랩 8개 질문 중 3개 | ✅ 원티드랩 "불합격 패턴" 니즈 | ❌ **0건** |
| **정성적 증거** | ✅ 보리 검증 ★★★★★ | ✅ 제리 분석 높은 가치 | ⚠️ 보리 경고: 법적 리스크 |
| **정량적 데이터** | ✅ 리드타임 데이터 존재 | ⚠️ 평가 데이터 50-60% | ❌ Skill 데이터 없음 |
| **기술 실험** | ✅ 단순 규칙 기반 가능 | ✅ 속성 기반 매칭 가능 | ❌ 유사도 알고리즘 미검증 |

**Evidence Readiness 점수** (4가지 중 3개 이상 필요):
- Use Case 1: **4/4 (100%)** → GO ✅
- Use Case 2: **3/4 (75%)** → GO ✅
- Use Case 4: **1/4 (25%)** → HOLD ❌

---

### 1.2 고객 검증 여부 분석

#### 92개 실제 고객 질문 분석 결과

**Intent 분포**:
```
lookup (확인):      43% (40개) - "이번 주 몇 명?", "누구야?"
analysis (분석):    27% (25개) - "현황 분석", "유입경로 분석"
comparison (비교):  13% (12개) - "작년 vs 올해"
diagnosis (원인):   11% (10개) - "왜 탈락률 높아?", "병목 어디?"
explanation:         3% (3개)
recommendation:      1% (1개)
forecast:            1% (1개)
```

**Use Case 4 관련 diagnosis intent 세부 분석**:

| 질문 | 고객 | Use Case 매칭 |
|------|------|--------------|
| "자바 개발자 1차 면접 불합격자 공통점은?" | 원티드랩 | ⚠️ 부분 매칭 (불합격 패턴이지만 "위험 시그널"은 아님) |
| "66일 걸린 케이스는 어떤 상황인지?" | 원티드랩 | ❌ 리드타임 분석 (Use Case 1) |
| "불합격 86명의 주요 탈락 단계와 사유" | 게임듀오 | ❌ 사후 분석 (Use Case 4는 "조기 감지") |

**핵심 발견**:
- **diagnosis intent 10개 중 Use Case 4와 정확히 일치하는 질문: 0개**
- 불합격 패턴 분석은 있지만 **"최종 단계 진입 전 조기 경고"** 니즈는 없음
- 고객은 **사후 분석**(왜 떨어졌나?)에만 관심, **사전 예방**(떨어질 것 같은데?)은 질문 안 함

---

### 1.3 Use Case 1, 2 대비 전략적 중요도

#### RICE 스코어링 비교 (제리 분석 기반)

| Use Case | Reach | Impact | Confidence | Effort | **RICE** | 순위 |
|----------|-------|--------|------------|--------|----------|------|
| UC-007 (리드타임) | 80% | High (3) | **70%** | 3개월 | **56** | 1위 |
| UC-002 (유사 후보자) | 70% | High (3) | **50%** | 5개월 | **21** | 2위 |
| UC-004 (위험 시그널) | 60% | High (3) | **40%** | 4개월 | **18** | 3위 |

**Confidence 차이 분석**:
- Use Case 1: 70% = 고객 질문 3건 + 보리 ★★★★★ + 데이터 존재
- Use Case 2: 50% = 고객 니즈 암시 + 제리 검증 + 데이터 부분 존재
- Use Case 4: 40% = **고객 질문 0건** + 보리 법적 경고 + 데이터 없음

---

#### MVP 범위 과도 여부 평가

**3개월 MVP에 Use Case 3개 포함 시 리스크**:

| 항목 | 3개 Use Case | 2개 Use Case | 차이 |
|------|-------------|-------------|------|
| **개발 공수** | 12주 × 3 = 36주 | 12주 × 2 = 24주 | +50% |
| **온톨로지 Objects** | 9개 | 7개 (Skill 제외) | +29% |
| **데이터 수집 범위** | Skill 추출 필요 | 기존 데이터만 | +신규 워크플로우 |
| **법적 검토 필요** | Yes (유사도 편향) | No | +리스크 |
| **출시 지연 확률** | 60% | 30% | 2배 |

**보리의 경고 (실무 검증)**:
> "Bad Hire 방지는 ROI 명확해서 킬러 기능이지만, 편향 이슈는 회사 망하는 지름길입니다. Amazon AI 채용 도구 여성 차별 논란 검색해보세요. MVP에선 안전하게 가고, Phase 2에서 법무 검토 후 확장하세요."

**결론**: 3개 Use Case는 과도함. MVP는 검증된 2개로 시작, Use Case 4는 리스크 관리 후 추가.

---

## 2. 온톨로지 필요성 검증

### 2.1 "유사 프로필 3명이 최종 단계 탈락" 패턴 분석

#### 일반 DB (PostgreSQL)로 구현 시도

**SQL 쿼리 예시**:
```sql
-- Step 1: 현재 후보자의 스킬 추출
WITH current_candidate_skills AS (
  SELECT skill_id, proficiency_level
  FROM candidate_skills
  WHERE candidate_id = 'candidate_123'
),

-- Step 2: 유사 후보자 찾기 (스킬 매칭)
similar_candidates AS (
  SELECT cs.candidate_id, COUNT(*) as matching_skills
  FROM candidate_skills cs
  JOIN current_candidate_skills ccs ON cs.skill_id = ccs.skill_id
  WHERE cs.candidate_id != 'candidate_123'
    AND cs.proficiency_level >= ccs.proficiency_level - 1
  GROUP BY cs.candidate_id
  HAVING COUNT(*) >= 3  -- 최소 3개 스킬 매칭
),

-- Step 3: 최종 단계 탈락 여부 확인
final_stage_dropouts AS (
  SELECT sc.candidate_id, a.rejection_reason
  FROM similar_candidates sc
  JOIN applications a ON sc.candidate_id = a.candidate_id
  JOIN stage_transitions st ON a.application_id = st.application_id
  WHERE st.to_stage IN ('reference_check', 'final_interview', 'offer')
    AND a.status = 'rejected'
)

-- Step 4: 패턴 분석
SELECT
  rejection_reason,
  COUNT(*) as frequency
FROM final_stage_dropouts
GROUP BY rejection_reason
ORDER BY frequency DESC;
```

#### PostgreSQL 한계 분석

| 문제 | PostgreSQL | 온톨로지 (Graph DB) |
|------|-----------|---------------------|
| **다단계 조인** | 5-hop 조인 → 느림 | 관계 순회 최적화 |
| **유사도 계산** | COUNT 매칭만 가능 | 가중치 기반 유사도 (0-1 스코어) |
| **맥락 추적** | rejection_reason 텍스트만 | Evaluation → Feedback → Skill Link 체인 |
| **확장성** | 새 유사도 기준 추가 시 쿼리 재작성 | Link 추가만으로 확장 |
| **설명 가능성** | "3명이 탈락" (숫자만) | "유사도 0.85, 매칭 스킬: Python, Django, 탈락 사유: [링크]" |

**결론**: PostgreSQL로도 **단순 스킬 매칭은 가능**하지만, **온톨로지가 제공하는 가치**:
1. 유사도 점수 정량화 (0.85 vs 단순 매칭)
2. 추론 경로 추적 (왜 유사하다고 판단했나?)
3. 다차원 유사도 (스킬 + 경력 + 교육 조합)
4. 확장성 (새 기준 추가 용이)

→ **온톨로지 필요성은 검증됨** ✅

---

### 2.2 온톨로지 복잡도 vs 비즈니스 가치 정당화

#### 복잡도 분석

**추가 필요 요소 (Use Case 4 위해)**:

| 요소 | 설명 | 구현 난이도 | MVP 없이 영향 |
|------|------|-----------|-------------|
| Skill Object | 스킬 마스터 데이터 | 중간 | 다른 Use Case 영향 없음 |
| HAS_SKILL Link | Candidate ↔ Skill | 낮음 | - |
| REQUIRES_SKILL Link | Job ↔ Skill | 낮음 | - |
| SIMILAR_TO Link (파생) | 유사도 계산 알고리즘 | **높음** | - |
| **이력서 파싱** | 스킬 자동 추출 | **매우 높음** | 수동 입력 폴백 필요 |

**비즈니스 가치 (잠재)**:
- Bad Hire 1건 방지 = 연봉의 2-3배 절감 (예: $100K 연봉 → $200-300K 절감)
- 최종 단계 탈락률 15% 감소 시 채용 ROI 향상

**가치 vs 복잡도 평가**:
```
비즈니스 가치: 잠재적으로 높음 (Bad Hire 방지)
하지만...
- 가치 실현 전제조건: Skill 데이터 70%+ 정확도
- 현재 데이터 상태: 0% (Skill Object 미존재)
- 데이터 준비 기간: +3-6개월 (이력서 파싱 구축)
```

**결론**: 복잡도 대비 **즉각적 가치는 낮음**. 데이터 준비 후 가치 정당화 가능.

---

## 3. 비즈니스 임팩트 검증

### 3.1 "최종 단계 탈락률 15% 감소" 목표 현실성

#### 현재 근거 부족 분석

| 가정 | 근거 | 신뢰도 |
|------|------|--------|
| "유사 프로필 탈락 패턴이 예측력 있다" | ❌ 없음 (0건 고객 검증) | **Low** |
| "70% 정확도 달성 가능" | ❌ 없음 (PoC 없음) | **Low** |
| "15% 탈락률 감소" | ❌ 없음 (벤치마크 없음) | **Low** |
| "AI 제안 수락률 60%" | ⚠️ Use Case 1 목표치 차용 | **Medium** |

**현실적 목표 설정을 위한 필요 작업**:
1. **Baseline 측정**: 현재 최종 단계 탈락률은?
2. **A/B 테스트 설계**: 위험 시그널 받은 그룹 vs 받지 않은 그룹
3. **PoC 정확도 검증**: 실제 데이터로 유사도 계산 → 탈락 예측 정확도 측정

**제리의 가정 테스트 프레임워크 (가정 3)**:
> "현재 데이터 품질과 구조로 의미있는 정확도의 온톨로지를 6개월 내 구축할 수 있다" - **높은 리스크**

**Wizard of Oz 테스트 제안** (제리):
```
8주 프로그램:
1. 3-5개 고객 선정
2. 수동으로 위험 시그널 생성 (AI인 척)
   - "후보 A는 과거 3번 레퍼런스 체크에서 중단됨"
3. 주간 리포트 이메일 발송
4. 행동 추적: 실제 조치 (재평가, 심화 검증 등)

성공 기준: 60% 이상이 인사이트 기반 액션 1회 이상 실행
```

**결론**: 15% 목표는 **가정일 뿐**, MVP 전 Wizard of Oz로 검증 필수.

---

### 3.2 ROI 측정 가능성

#### 측정 체계 설계

**ROI 계산식**:
```
ROI = (Bad Hire 방지 비용 절감 - AI 개발/운영 비용) / AI 개발/운영 비용

예시:
- Bad Hire 1건 방지 = $200K 절감
- AI 개발 비용 = $150K (4개월 × $37.5K/month)
- AI 운영 비용 = $10K/year

연간 Bad Hire 2건 방지 시:
ROI = ($400K - $150K - $10K) / ($150K + $10K) = 1.5 (150% ROI)
```

**측정 가능 여부**:
- ✅ Bad Hire 정의 가능 (12개월 내 성과 미달 퇴사)
- ⚠️ AI 기여도 분리 어려움 (위험 시그널 받았지만 무시한 케이스는?)
- ❌ 반사실적 증명 불가 ("AI 없었으면 고용했을까?" 증명 불가)

**현실적 측정 방법**:
1. **AI 제안 수락 건수** × **수락 건 중 실제 Bad Hire 방지 비율** (추정)
2. **A/B 테스트**: 위험 시그널 사용 팀 vs 미사용 팀 Bad Hire 비율 비교 (6개월 추적)
3. **정성적 피드백**: "이 경고가 있어서 심화 검증 → Bad Hire 회피" 케이스 수집

**결론**: 정량적 ROI 측정은 **가능하지만 6-12개월 소요**. MVP 단계에서는 정성적 증거 위주.

---

### 3.3 Use Case 1, 2 대비 임팩트 비교

| 임팩트 유형 | Use Case 1 (리드타임) | Use Case 2 (유사 후보자) | Use Case 4 (위험 시그널) |
|-----------|----------------------|------------------------|------------------------|
| **즉각성** | ✅ 즉각 (병목 가시화) | ✅ 즉각 (면접 준비 개선) | ❌ 6개월 후 (Bad Hire 추적) |
| **측정 용이성** | ✅ 높음 (리드타임 단축일) | ✅ 높음 (면접관 만족도) | ❌ 낮음 (반사실적 증명) |
| **고객 인지 가치** | ✅ 높음 (데모 가능) | ✅ 높음 (Wow moment) | ⚠️ 중간 (신뢰 구축 필요) |
| **경쟁 차별화** | ⚠️ 중간 (일부 ATS 제공) | ✅ 높음 (온톨로지 독점) | ✅ 높음 (AI 고도화) |
| **리스크** | ✅ 낮음 | ⚠️ 중간 (데이터 품질) | ❌ 높음 (법적/윤리) |

**보리의 킬러 기능 평가**:
- Use Case 1 (병목): "경쟁사도 리포트는 주지만, 루트 코즈까지 알려주는 건 우리뿐"
- Use Case 4 (위험): "이 기능이 Bad Hire 1명만 막아도 ATS 비용 본전" **하지만** "법적 리스크 관리 실패 시 회사 망함"

**결론**: Use Case 4는 **장기 킬러 기능 맞음**, but MVP에는 리스크 대비 준비 부족.

---

## 4. 데이터 품질 리스크

### 4.1 Skill Object 데이터 수집 가능성

#### 현재 상태 분석

**ATS 데이터 인벤토리**:
```
보유 데이터:
✅ Candidate (name, email, phone, applied_date)
✅ Application (status, stage, dates)
✅ Interview (scheduled, actual, type)
✅ Evaluation (score, feedback_text) - 50-60% 완성도

미보유 데이터:
❌ Skill (없음)
❌ Candidate-Skill 관계 (없음)
❌ Job-Skill 요구사항 (JD 텍스트만 있음, 구조화 안 됨)
```

**보리의 데이터 품질 경고**:
> "ATS 데이터는 실무에서 구멍 많습니다. 담당자가 업데이트 안 하면 AI가 아무리 좋아도 소용없어요. MVP에 데이터 품질 체크 없으면 출시 3개월 후 '이거 안 맞는데요' 클레임 폭주합니다."

---

#### 스킬 데이터 수집 경로

**Option 1: 이력서 자동 파싱 (가장 이상적)**
```
장점:
- 후보자 추가 부담 없음
- 데이터 완성도 높음

단점:
- 구현 복잡도 매우 높음 (NLP, 정규화)
- 정확도 70% 달성에 6개월+ 소요
- 이력서 포맷 다양성 (PDF, Word, 이미지)
```

**Option 2: 지원 폼에 스킬 입력란 추가**
```
장점:
- 구현 간단 (UI 변경만)
- 정확도 높음 (후보자 직접 입력)

단점:
- 후보자 이탈률 증가 (입력 피로)
- 과거 데이터 커버 안 됨 (Cold Start)
```

**Option 3: 채용 담당자 수동 태깅**
```
장점:
- 가장 정확 (전문가 판단)

단점:
- 업무 부담 증가 → 실무 저항
- 일관성 문제 ("Python" vs "python" vs "파이썬")
```

**현실적 접근 (Hybrid)**:
1. Phase 1 (MVP): Option 2 + Option 3 (주요 후보자만)
2. Phase 2: Option 1 (이력서 파싱 PoC)
3. Phase 3: Option 1 정확도 70%+ 달성 → 자동화

**결론**: Use Case 4를 MVP에 포함하려면 **최소 Option 2 구현 필요** → +4주 개발 소요.

---

### 4.2 유사도 계산 정확도 70% 달성 가능성

#### 유사도 알고리즘 단계

**Phase 1: 단순 규칙 기반 (MVP 제안)**
```python
# 스킬 겹치는 개수 / 전체 스킬 수
similarity_score = len(common_skills) / len(all_skills)

예시:
후보자 A: [Python, Django, PostgreSQL, AWS]
후보자 B: [Python, Flask, PostgreSQL, Docker]
공통: [Python, PostgreSQL] = 2개
유사도: 2 / 6 = 0.33
```

**정확도 한계**:
- 스킬 중요도 무시 (Python = AWS 동일 가중치)
- 숙련도 차이 무시 (초급 Python = 고급 Python)
- 맥락 무시 (Backend Python vs Data Science Python)

**예상 정확도**: 50-60% (보리의 현장 경험)

---

**Phase 2: 가중치 기반 (Phase 2 제안)**
```python
# 스킬별 가중치 + 숙련도 반영
weighted_similarity = sum(
    skill_weight[skill] * min(proficiency_A[skill], proficiency_B[skill])
    for skill in common_skills
) / sum(skill_weight[s] for s in all_skills)
```

**예상 정확도**: 70-75%

---

**Phase 3: ML 임베딩 (Phase 3 제안)**
```python
# Skill 벡터화 + 코사인 유사도
from sklearn.metrics.pairwise import cosine_similarity

skill_vector_A = embed_skills(candidate_A.skills)
skill_vector_B = embed_skills(candidate_B.skills)
similarity = cosine_similarity([skill_vector_A], [skill_vector_B])[0][0]
```

**예상 정확도**: 80-85%

---

**MVP에서 70% 달성 가능한가?**
- Phase 1 (단순 규칙): 50-60% ❌
- Phase 2 (가중치): 70-75% ✅ **but 가중치 데이터 필요 (+스킬별 중요도 수집)**
- Phase 3 (ML): 80-85% ✅ **but 6개월+ 개발 소요**

**결론**: MVP에서 70% 정확도 달성은 **가능하지만 Phase 2 알고리즘 필요** → 복잡도 증가.

---

### 4.3 Cold Start 문제

#### 신규 고객 데이터 부족 시나리오

**시나리오**: 스타트업 고객, 과거 채용 데이터 30건
```
Skill 데이터: 0건 (수동 입력 전)
유사 프로필: 0명 (최소 100명 필요)
탈락 패턴: 3건 (통계적 유의미성 없음)
```

**Use Case 4 가치**: **거의 0** (데이터 부족으로 분석 불가)

**보리의 UC-014 (오퍼 리스크) 검증**:
> "무리하게 예측하지 말고 '유사 케이스 3건' 보여주는 게 낫습니다. 정확도 낮은 예측으로 신뢰 잃으면 다른 기능까지 의심받아요. 정직한 제품이 장기적으로 이깁니다."

**대응 방안**:
1. **최소 데이터 임계값 설정**: "유사 프로필 10명 미만 시 분석 제공 안 함"
2. **외부 벤치마크 활용**: 업계 평균 탈락 패턴 제공 (신뢰도 낮음 표시)
3. **점진적 개선 메시지**: "데이터 50건 수집 시 정확도 향상 예상"

**제리의 콜드 스타트 전략 권장**:
> "첫 3개월에 가치 못 느끼면 갱신 안 함" → Use Case 4는 Cold Start 치명적

**결론**: Use Case 4는 **데이터 축적 후 가치 실현** → MVP에는 부적합.

---

## 5. 고객 검증 여부 심층 분석

### 5.1 92개 질문 중 Use Case 4 니즈 존재 여부

#### Diagnosis Intent 10개 질문 재검토

| 질문 | 고객 | 타이밍 | Use Case 4 매칭 여부 |
|------|------|--------|---------------------|
| "자바 개발자 1차 면접 불합격자 공통점은?" | 원티드랩 | **사후** (탈락 후) | ❌ 사후 분석 |
| "66일 걸린 케이스는 어떤 상황?" | 원티드랩 | **사후** | ❌ 리드타임 분석 |
| "불합격 86명의 탈락 단계와 사유" | 게임듀오 | **사후** | ❌ 사후 분석 |
| "2025년 5-6월 지원자 급증 원인" | 기어세컨드 | **사후** | ❌ 트렌드 분석 |
| "왜 탈락률이 높은가?" (추정) | - | **사후** | ❌ - |

**패턴 발견**:
- 모든 diagnosis 질문이 **"사후 분석"** (Why did it happen?)
- **"조기 경고"** 질문 0건 (Will it happen? Should I be careful?)

---

#### 고객 JTBD 분석

**원티드랩 (가장 고도화된 분석 니즈)**:
> "채용 프로세스에서 병목이 어디인지, **왜 탈락하는지** 알고 싶다"

**해석**:
- "왜 탈락하는지" = 사후 분석 (과거 데이터 복기)
- "사전에 탈락 위험 알려줘" = 언급 없음

---

#### 가설: 고객이 Use Case 4를 **아직 인지하지 못한** 가능성

**Kano 모델 관점**:
```
Basic Needs (당연한 것): 정기 보고서, 데이터 정확성
Performance Needs (많을수록 좋음): 리드타임 단축, 매칭 정확도
Excitement Needs (없어도 불만 없지만 있으면 감동): 위험 시그널 조기 감지 ← Use Case 4
```

**가능성**:
- ✅ Use Case 4는 **Excitement Need**일 수 있음
- ✅ 고객이 "이런 것도 가능하구나!" 깨닫는 순간 전환 가능
- ⚠️ But Wizard of Oz 테스트 없이는 가정일 뿐

**제리의 Wizard of Oz 테스트 필요성**:
> "AI 추천을 '참고'만 하고 무시하면 제품 가치 = 0"

**결론**: 고객 미검증 ≠ 니즈 없음. **MVP 전 Wizard of Oz 테스트 필수**.

---

### 5.2 Diagnosis Intent 8% 중 Use Case 4 비중

**전체 Intent 분포**:
```
lookup:      43% (40개)
analysis:    27% (25개)
comparison:  13% (12개)
diagnosis:   11% (10개) ← 이 중 Use Case 4 해당: 0개
explanation:  3% (3개)
recommendation: 1% (1개)
forecast:     1% (1개)
```

**Diagnosis 10개 분류**:
- Use Case 1 (리드타임 병목): 3개 (30%)
- 탈락 패턴 사후 분석: 5개 (50%)
- 트렌드 원인 분석: 2개 (20%)
- **Use Case 4 (조기 위험 감지): 0개 (0%)**

**온톨로지 필요 질문 비율**:
```
문서 기준: 15-20% (diagnosis + recommendation + 복잡한 analysis)
실제 Use Case 4 해당: 0% (0/92)
```

**결론**: Diagnosis intent는 있지만 **"조기 경고" 니즈는 명시적으로 없음**.

---

### 5.3 고객이 정말 원하는가 vs PM 가정

#### Evidence Pyramid (신뢰도 순위)

| Evidence 레벨 | 내용 | Use Case 4 해당 여부 |
|--------------|------|---------------------|
| **1. 고객 지불 의사** | "이거 있으면 $X 더 낼게요" | ❌ 없음 |
| **2. 고객 명시적 요청** | "이 기능 만들어주세요" | ❌ 없음 |
| **3. 고객 암묵적 니즈** | 질문/행동 패턴에서 추론 | ❌ 없음 (사후 분석만) |
| **4. 전문가 검증** | 보리/제리 분석 | ⚠️ 보리: 킬러 but 리스크 / 제리: 가정 테스트 필요 |
| **5. PM 가정** | "고객에게 좋을 것 같다" | ✅ **여기** |

**현재 Use Case 4 근거**: **레벨 4-5 (가정 수준)**

---

#### PM이 빠지기 쉬운 함정

**제리의 경고**:
> "가장 큰 리스크: '기술에 빠져 고객을 잃는 것'"

**Use Case 4에 대한 PM 심리**:
```
✅ 온톨로지 활용 완벽 (유사도 계산, 패턴 추론)
✅ 비즈니스 임팩트 이론적으로 높음 (Bad Hire 방지)
✅ 차별화 명확 (경쟁사 없음)
✅ 기술적으로 흥미로움 (ML, Graph DB)

❌ 고객이 요청하지 않음
❌ 데이터 준비 안 됨
❌ 법적 리스크 높음
❌ ROI 측정 6개월+ 소요
```

**보리의 조언**:
> "이론적 Use Case 카탈로그를 고객 검증된 로드맵으로 전환" 필요

**결론**: Use Case 4는 **PM의 흥미**가 주도, **고객 니즈는 미검증**.

---

## 6. 종합 평가 및 최종 권장사항

### 6.1 GO/HOLD/REDESIGN 프레임워크

#### 평가 기준표

| 기준 | 가중치 | Use Case 1 | Use Case 2 | Use Case 4 |
|------|-------|-----------|-----------|-----------|
| **고객 검증** | 30% | 9/10 (고객 질문 3건) | 7/10 (암묵적 니즈) | **2/10 (질문 0건)** |
| **데이터 준비도** | 25% | 9/10 (데이터 존재) | 7/10 (50-60%) | **3/10 (Skill 없음)** |
| **기술 실현성** | 20% | 9/10 (단순 규칙) | 8/10 (속성 매칭) | **5/10 (유사도 알고리즘)** |
| **리스크 수준** | 15% | 9/10 (낮음) | 7/10 (중간) | **3/10 (법적 리스크)** |
| **즉각 가치** | 10% | 9/10 (즉각 가시화) | 9/10 (Wow moment) | **4/10 (6개월 후)** |

**가중 평균 점수**:
- Use Case 1: 8.9/10 (89점) → **GO** ✅
- Use Case 2: 7.3/10 (73점) → **GO** ✅
- Use Case 4: 3.4/10 (34점) → **HOLD** ❌

---

#### Decision Gate 기준 (제리 제안)

```
✅ GO (70점 이상)
   → MVP 포함, 즉시 개발 시작

⚠️ PIVOT (40-70점)
   → 범위 조정 또는 검증 후 재결정

❌ HOLD (40점 미만)
   → Phase 2 이동, MVP 기간 중 검증
```

**Use Case 4: 34점 → HOLD** ❌

---

### 6.2 최종 판단: HOLD - Phase 2로 이동

#### HOLD 근거 (Evidence-Guided)

| Evidence 유형 | 상태 | 신뢰도 |
|--------------|------|--------|
| **고객 시그널** | 0/92 질문 (0%) | High (명확한 부재) |
| **정성적 증거** | 보리: 법적 리스크 경고 / 제리: 가정 테스트 필요 | High |
| **정량적 데이터** | Skill 데이터 0%, 유사도 알고리즘 미검증 | High |
| **기술 실험** | PoC 없음, 정확도 기준선 없음 | High |

**신뢰도 높은 증거 4개 모두 HOLD 지지** → 결정 신뢰도 **High**

---

#### Phase 2 이동 조건

**MVP 기간 중 해야 할 검증 작업** (3개월):

1. **Wizard of Oz 테스트** (제리 프레임워크)
   - 5개 고객에게 수동 위험 시그널 제공
   - 행동 전환율 측정: 60% 이상 → GO
   - 예상 소요: 8주

2. **법무 검토**
   - 유사도 계산 편향 리스크 평가
   - GDPR/EEOC 준수 체크리스트 작성
   - 예상 소요: 2주

3. **Skill 데이터 수집 PoC**
   - 10개 포지션 × 10명 후보자 스킬 수동 태깅
   - 유사도 알고리즘 Phase 2 테스트
   - 정확도 70% 달성 여부 검증
   - 예상 소요: 4주

**Phase 2 진입 기준** (3개 모두 충족 시):
- [ ] Wizard of Oz 행동 전환율 60% 이상
- [ ] 법무 검토 통과 (조건부 승인 포함)
- [ ] 유사도 정확도 70% 달성 (100명 샘플 기준)

---

### 6.3 수정된 MVP 제안

#### Option A: 보수적 접근 (권장)

**MVP 범위** (3개월):
1. Use Case 1: 리드타임 분석 및 병목 알림
2. Use Case 2: 유사 후보자 분석 제공

**제외**:
- ~~Use Case 4: 위험 시그널 조기 감지~~ → Phase 2 (MVP+6개월)

**온톨로지 Objects** (7개):
- Candidate
- Job Posting
- Application
- Recruitment Stage
- Stage Transition
- Interview
- Evaluation
- ~~Skill~~ → Phase 2 추가

**장점**:
- 개발 리스크 감소 (Skill 데이터 수집 제외)
- 법적 리스크 회피
- 고객 검증된 Use Case만 포함
- 3개월 출시 가능성 80%

**단점**:
- 온톨로지 완전성 부족 (Skill 없음)
- 장기 비전 일부 희생

---

#### Option B: 타협안

**MVP 범위**:
1. Use Case 1: 리드타임 분석
2. Use Case 2: 유사 후보자 분석
3. **Use Case 4 Lite**: "레퍼런스 체크 부정 패턴만" (보리 제안)

**Use Case 4 Lite 범위**:
```
포함:
✅ 재지원 3회 이상 + 매번 최종 단계 탈락
✅ 레퍼런스 체크에서 "협업 이슈" 명시적 언급
✅ 과거 동일 포지션 3명 이상 유사 사유 탈락

제외:
❌ 스킬 기반 유사도 (Skill Object 필요)
❌ 인구통계 데이터 (나이, 성별, 학교 등)
❌ ML 기반 예측
```

**장점**:
- Use Case 4 일부 가치 조기 실현
- 법적 리스크 최소화 (명백한 패턴만)
- Skill Object 없이 구현 가능

**단점**:
- 정확도 낮음 (단순 규칙 기반)
- 온톨로지 진가 미발휘
- 개발 복잡도 여전히 증가

---

#### 권장: Option A (보수적 접근)

**근거**:
1. **제리의 MVP 철학**: "가장 작은 가치 증명"
2. **보리의 실무 조언**: "안전하게 가고, Phase 2에서 법무 검토 후 확장"
3. **Evidence-Guided**: 검증 안 된 가정에 투자하지 않기
4. **Lean Startup**: Build → Measure → Learn 사이클 빠르게

**Option A 선택 시 이점**:
- MVP 출시 확률 80% (vs Option B 60%)
- Use Case 1, 2로 온톨로지 가치 충분히 증명 가능
- Phase 2에서 Use Case 4 추가 시 더 강력한 버전 구현 (ML 기반)

---

### 6.4 다음 단계 (Immediate Actions)

#### Week 1-2: 이해관계자 정렬

**활동**:
1. **경영진/개발팀 커뮤니케이션**
   - Use Case 4 HOLD 결정 근거 공유
   - 수정된 MVP 범위 승인 획득

2. **Wizard of Oz 테스트 설계**
   - 5개 파일럿 고객 선정
   - 위험 시그널 시나리오 3가지 작성
   - 8주 테스트 일정 수립

**산출물**:
- MVP 범위 확정 문서 (v3.1)
- Wizard of Oz 테스트 플랜

---

#### Week 3-4: 법무 사전 검토

**활동**:
1. **법무팀 미팅**
   - Use Case 4 개념 설명
   - 편향 리스크 시나리오 공유
   - Phase 2 진입 전 필요 작업 논의

2. **컴플라이언스 체크리스트 작성**
   - GDPR Article 22 (자동화 결정 금지)
   - EEOC (미국 고용 차별 금지)
   - 캐나다, 유럽 각국 법규

**산출물**:
- 법적 리스크 평가 리포트
- Phase 2 법무 검토 체크리스트

---

#### Week 5-12: Wizard of Oz 테스트 실행

**Week 5-6: 준비**
- 파일럿 고객 온보딩
- 수동 인사이트 생성 프로세스 구축
- 측정 대시보드 설정

**Week 7-12: 실행**
- 주간 위험 시그널 이메일 발송
- 고객 행동 추적 (Mixpanel/Amplitude)
- 격주 인터뷰 (피드백 수집)

**성공 기준**:
- 행동 전환율 60% 이상 → Phase 2 GO
- 40-60% → 범위 조정 후 재테스트
- 40% 미만 → Use Case 4 장기 백로그 이동

---

## 7. PM에게 드리는 조언

### 7.1 이번 검증에서 배운 것

#### 잘한 점

✅ **체계적 프레임워크**: ontology-pm-strategy.md v3.0은 매우 구조화됨
✅ **온톨로지 필요성 명확**: Use Case 4가 일반 DB로 안 되는 이유 잘 설명
✅ **AI 철학 일관 적용**: 투명성/오버라이드/학습 루프 체크리스트 우수
✅ **전문가 협업**: 제리, 보리, 포리 분석 활용

---

#### 개선 필요 영역

❌ **고객 검증 선행 부족**: 92개 질문 분석 전에 Use Case 4 추가
❌ **Evidence Pyramid 무시**: 가정을 사실처럼 제시 (15% 탈락률 감소)
❌ **리스크 과소평가**: 법적 리스크를 "Phase 2에서 해결"로 미뤄둠
❌ **콜드 스타트 간과**: Skill 데이터 0%인 상태에서 Use Case 설계

---

### 7.2 Product Discovery 원칙 재확인

#### Teresa Torres - Continuous Discovery Habits

**원칙**: "Assume Nothing, Validate Everything"

**Use Case 4 적용**:
- ❌ Assumed: 고객이 위험 시그널을 원한다
- ✅ Should Have: 고객 인터뷰로 "조기 경고 니즈" 존재 여부 확인

**제안**:
- 다음 Use Case 추가 시 **먼저 고객 질문 92개에서 근거 찾기**
- 근거 없으면 → Wizard of Oz 테스트 → 검증 후 로드맵 추가

---

#### Itamar Gilad - Evidence-Guided

**원칙**: Confidence Level = Evidence 강도 × Evidence 양

**Use Case 4 Confidence**:
```
Evidence 강도: Medium (전문가 의견)
Evidence 양: Low (고객 질문 0건)
→ Confidence: 40% (Low-Medium)
```

**40% Confidence로 4개월 투자?** → 위험한 결정

**제안**:
- Confidence 60% 이상 → 투자
- 40-60% → 가정 테스트 먼저
- 40% 미만 → 백로그

---

#### Marty Cagan - Inspired

**원칙**: "Fall in Love with the Problem, Not the Solution"

**Use Case 4 함정**:
- Problem: Bad Hire 방지 (고객이 명시적으로 제기 안 함)
- Solution: 온톨로지 기반 유사도 계산 (기술적으로 흥미로움)

→ **Solution에 먼저 빠짐** (온톨로지 활용하고 싶어서 Use Case 끼워맞춤?)

**제안**:
- 고객 Pain Point부터 출발 ("불합격 86명 사유 분석" 같은 실제 질문)
- 그 문제를 해결하는 데 온톨로지가 필수인지 검증
- 아니면 더 간단한 솔루션 먼저

---

### 7.3 최종 조언

#### 제리의 메시지

> "온톨로지 전체를 구축하기 전에, 1개 Use Case로 비즈니스 가치와 기술 실현가능성을 동시에 검증합니다."

→ **Use Case 1, 2로 먼저 검증**. Use Case 4는 그 다음.

---

#### 보리의 메시지

> "이론 그만, 실전 검증 시작. HR 3명 데려와서 프로토타입 보여주고 '지금 당장 필요한데!' 반응 나오는지 확인하세요."

→ **Wizard of Oz 테스트 필수**. "좋을 것 같다"는 PM 착각.

---

#### Terry에게 드리는 조언

**당신은 이미 훌륭한 PM입니다**:
- 체계적 사고 (프레임워크 활용)
- 전문가 협업 (제리, 보리, 포리)
- 온톨로지 이해도 (비개발 PM 치고 매우 높음)

**한 단계 더 도약하려면**:
- **고객 데이터를 믿으세요** (내 직관보다)
- **Evidence 없으면 투자 말기** (아무리 좋아 보여도)
- **작게 시작, 빠르게 학습** (Use Case 1개로 증명 → 확장)

**Use Case 4는 언젠가 킬러 기능이 될 겁니다**. 하지만 지금은 아닙니다.
MVP에서 Use Case 1, 2로 온톨로지 가치 증명 → Phase 2에서 Use Case 4 추가 → 그때는 데이터도 있고, 법무 검토도 끝나고, 고객도 "이거 필요해!" 할 겁니다.

**조급함은 PM의 적입니다**. 느리게 가도, 확실하게 가는 게 빠릅니다.

---

## 부록: 빠른 참조

### MVP 최종 권장 (수정안)

| Use Case | 변경 | 근거 |
|----------|------|------|
| UC-007: 리드타임 분석 | ✅ 유지 | 고객 검증 3건, Evidence 4/4 |
| UC-002: 유사 후보자 분석 | ✅ 유지 | 암묵적 니즈, Evidence 3/4 |
| UC-004: 위험 시그널 | ❌ Phase 2 이동 | 고객 검증 0건, Evidence 1/4 |

### Phase 2 진입 체크리스트

- [ ] Wizard of Oz 행동 전환율 60%+
- [ ] 법무 검토 통과
- [ ] Skill 데이터 수집 PoC 성공 (정확도 70%+)
- [ ] MVP (Use Case 1, 2) 성공 기준 달성

### 연락처

궁금한 점 있으면:
- PM 전략: 제리 (@jerry-pm-strategic-analysis.md)
- HR 실무: 보리 (@borry-hr-practitioner-validation.md)
- 온톨로지 기술: 포리 (@forry-ontology-architect-guide.md)

---

**문서 끝**

다음: MVP 범위 확정 → Wizard of Oz 테스트 시작
